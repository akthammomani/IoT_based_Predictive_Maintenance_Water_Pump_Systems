# -*- coding: utf-8 -*-
"""Failure_cause_Modeling_Classification_IoT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KbJhv1aqzrX9yOeb7MzoqcNZggGNoNui

# **Failure Cause Classification: Predictive Maintenance of Water Pump Systems**

* **Group 9:** Aktham Almomani
* **Course:** Data Analytics and Internet of Things (AAI-530-04)/ University Of San Diego
* **Semester:** Spring 2025

## **Contents**<a is='Contents'></a>
* [Introduction](#Introduction)
* [Dataset](#Dataset)
* [Setup and preliminaries](#Setup_and_preliminaries)
  * [Import Libraries](#Import_libraries)
  * [Helper functions](#Helper_Functions)
* [Importing the dataset](#Importing_the_dataset)
* [Failure Cause Transformation](#Failure_Cause_transformation)
* [Extracting Time-Based Features](#Extracting_Time_Based_Features)
* [Encoding Failure Causes](#Encoding_Failure_Causes)
* [Correlation](#Correlation)
* [Selecting the features](#Selecting_the_features)
* [Splitting the Data for Training and Testing](#Splitting_Data)
* [Scaling the Features](#Scaling_Features)
* [Training Machine Learning Model](#modeling)
* [Model Evaluation](#Model_Evaluation)
* [Features Importance](#Features_Importance)
* [Saving the prediction dataset](#Saving_prediction)

## **Introduction**<a id='Introduction'></a>
[Contents](#Contents)

Before training our model, we need to prepare the dataset to ensure we capture useful patterns in the data. Here's the plan:

* Extract Time-Based Features
Failures may have patterns based on time of day, month, or hour, so we'll extract:

 * Day: Some failures could be more frequent on certain days.
 * Month: Seasonal or long-term trends in failures.
 * Hour: Failures could correlate with operational loads (e.g., peak usage hours).
* Select Key Sensors as Features (X)
We'll use our selected key sensors to help classify failures. The following sensors are included:

 * snr_01_motor_phase_current
 * snr_02_motor_shaft_power
 * snr_03_motor_speed
 * snr_04_pump_impeller_speed
 * snr_05_pump_casing_vibration
 * snr_06_pump_discharge_pressure
 * snr_07_pump_lube_oil_supply_temp
 * snr_08_motor_casing_vibration
 * snr_09_motor_phase_voltage
* Define the Target Variable (y)
Our target variable is failure_cause, which classifiesfailure causes.

## **Dataset**<a id='Dataset'></a>
[Contents](#Contents)

The dataset we'll be using is called "Pump Sensor Data" and is available on [Kaggle](https://www.kaggle.com/datasets/nphantawee/pump-sensor-data/data). This dataset provides real-world IoT sensor data collected from water pump systems.

**Data Collection**: The data was gathered from sensors installed in water pump systems. These sensors monitor a wide range of parameters such as motor and pump vibrations, frequencies, power usage, and bearing temperatures. The collection process spans over time, capturing changes in system behavior and helping identify operational issues or equipment failures.

**Dataset Overview**:
* Number of Observations: The dataset contains 220,320 entries.
* Number of Variables: There are 55 columns, including 52 numerical variables (sensor readings), one timestamp column, and one categorical column representing machine status

## **Setup and preliminaries**<a id='Setup_and_preliminaries'></a>
[Contents](#Contents)

### **Import libraries**<a id='Import_libraries'></a>
[Contents](#Contents)
"""

#Let's import the necessary packages:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import accuracy_score, confusion_matrix

# let's run below to customize notebook display:
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.max_colwidth', 4000) # 100 means 100 characters in the col

# format floating-point numbers to 2 decimal places:
pd.set_option('float_format', '{:.4f}'.format)

"""### **Helper functions**<a id='Helper_Functions'></a>
[Contents](#Contents)
"""

def summarize_df(df):
    """
    Generate a summary DataFrame for an input DataFrame:
      - 'unique_count': No. unique values in each column.
      - 'data_types': Data types of each column.
      - 'missing_counts': No. of missing (NaN) values in each column.
      - 'missing_percentage': Percentage of missing values in each column.
    """
    # No. of unique values for each column:
    unique_counts = df.nunique()
    # Data types of each column:
    data_types = df.dtypes
    # No. of missing (NaN) values in each column:
    missing_counts = df.isnull().sum()
    # Percentage of missing values in each column:
    missing_percentage = 100 * df.isnull().mean()
    # Concatenate the above metrics:
    summary_df = pd.concat([unique_counts, data_types, missing_counts, missing_percentage], axis=1)
    # Rename the columns for better readibility:
    summary_df.columns = ['unique_count', 'data_types', 'missing_counts', 'missing_percentage']
    # Return summary df:
    return summary_df
#-----------------------------------------------------------------------------------------------------------------#
def value_counts_with_percentage(df, column_name):
    # Calculate value counts:
    counts = df[column_name].value_counts(dropna=False)

    # Calculate percentages:
    percentages = df[column_name].value_counts(dropna=False, normalize=True) * 100

    # Combine into a DataFrame and reset index:
    result = pd.DataFrame({
        'Count': counts,
        'Percentage': percentages
    }).rename_axis(None).reset_index()

    # Rename the first column to match the original column name:
    result.rename(columns={'index': column_name}, inplace=True)

    return result
#-----------------------------------------------------------------------------------------------------------------#
def dynamic_impute(df, threshold=0.1):
    """
    Impute missing values dynamically based on the gap between mean and median.
    """
    # Get sensor columns:
    sensor_columns = [col for col in df.columns if col.startswith('snr')]

    for col in sensor_columns:
        mean_val = df[col].mean()
        median_val = df[col].median()

        # Calculate percentage difference:
        diff = abs(mean_val - median_val) / mean_val

        # Imputation decision:
        if diff <= threshold:
            # Mean and median are close, use the mean:
            df[col] = df[col].fillna(mean_val)
            print(f"Imputed '{col}' using mean (diff = {diff:.2%})")
        elif median_val > mean_val:
            # Median is significantly higher, use the median:
            df[col] = df[col].fillna(median_val)
            print(f"Imputed '{col}' using median because median > mean (diff = {diff:.2%})")
        else:
            # Use median for any other skewed cases:
            df[col] = df[col].fillna(median_val)
            print(f"Imputed '{col}' using median (diff = {diff:.2%})")

    return df

"""## **Importing the dataset**<a id='Importing_the_dataset'></a>
[Contents](#Contents)

Since I'm using google colab, let's mount the driver:
"""

from google.colab import drive
drive.mount('/content/drive')

#First, let's load the clean sensor dataset : Data wrangling and preprocessing output
df = pd.read_csv(
    '/content/drive/My Drive/Colab Notebooks/sensor_clean_df.csv'
)

#now, let's look at the shape of df:
shape = df.shape
print("Number of rows:", shape[0], "\nNumber of columns:", shape[1])

# Here's, let's call again our helper function "summarize_df" to get more overall look about the columns in the dataset:
summarize_df(df)

# Now, let's look at the top 5 rows of the df:
df.head()

"""## **Failure Cause Transformation**<a id='Failure_Cause_transformation'></a>
[Contents](#Contents)

In here, I take failure_cause, my target variable, and reduce its 18 unique categories down to just 6 broader classes. This transformation helps streamline the classification process, making it easier to train models, interpret results, and identify key failure trends without unnecessary complexity. By grouping similar failure types together, I ensure the dataset remains informative while improving model performance and analysis efficiency.

"""

# Filter for abnormal machine status only
df_failure = df[df["failure_cause"] != "no_failure"].copy().reset_index(drop=True)

# Alright, let's fix up 'timestamp' to be datetime:
df_failure['timestamp']=pd.to_datetime(df_failure['timestamp'])

# Define the date range for the subset
start_date = "2018-04-26"
end_date = "2018-05-31"

# Filter the dataset
df_failure = df_failure[(df_failure["timestamp"] >= start_date) & (df_failure["timestamp"] <= end_date)].copy()

#now, let's look at the shape of df_failure:
shape = df_failure.shape
print("Number of rows:", shape[0], "\nNumber of columns:", shape[1])

value_counts_with_percentage(df_failure, 'failure_cause')

# Mapping function:
def map_failure_classes(failure):
    if failure == "rotational_issue, vibration_issue, electrical_issue":
        return "rotational_issue, vibration_issue, electrical_issue"
    elif failure == "vibration_issue, electrical_issue":
        return "vibration_issue, electrical_issue"
    elif failure == "vibration_issue":
        return "vibration_issue"
    elif failure == "rotational_issue, vibration_issue":
        return "rotational_issue, vibration_issue"
    elif failure == "rotational_issue":
        return "rotational_issue"
    else:
        return "Others"

# Apply transformation:
df_failure["failure_cause_v1"] = df_failure["failure_cause"].apply(map_failure_classes)
value_counts_with_percentage(df_failure, 'failure_cause_v1')

"""Alright, that looks better, 6 classes better than 18 classes!!

## **Extracting Time-Based Features**<a id='Extracting_Time_Based_Features'></a>
[Contents](#Contents)

Since this is a time-series problem, we want to capture patterns based on day, month, and hour. This helps our model understand when failures are more likely to happen.
"""

# Extract time-based features:
df_failure["day"] = df_failure["timestamp"].dt.day
df_failure["month"] = df_failure["timestamp"].dt.month
df_failure["hour"] = df_failure["timestamp"].dt.hour

df_failure.head()

"""## **Encoding Failure Causes**<a id='Encoding_Failure_Causes'></a>
[Contents](#Contents)

In here, I convert failure_cause_v1 into numerical labels using a mapping approach to optimize it for machine learning models. By reducing 18 failure categories into 4 distinct classes, this transformation improves model performance, enhances interpretability, and ensures efficient processing. This step is essential for handling categorical data in classification tasks.
"""

value_counts_with_percentage(df_failure, 'failure_cause_v1')

# First, let's Convert failure_cause_v1 to binary values:
df_failure["failure_cause_v2"] = df_failure["failure_cause_v1"].map({
    "rotational_issue, vibration_issue, electrical_issue": 0,
    "vibration_issue, electrical_issue": 1,
    "vibration_issue": 2,
    "rotational_issue, vibration_issue": 3,
    "rotational_issue": 4,
    "Others": 5
})

value_counts_with_percentage(df_failure, 'failure_cause_v2')

"""## **Correlation**<a id='Correlation'></a>
[Contents](#Contents)


"""

corr_df = df_failure[[
    "day", "month", "hour",
     "snr_01_motor_phase_current", "snr_02_motor_shaft_power", "snr_03_motor_speed",
     "snr_04_pump_impeller_speed", "snr_05_pump_casing_vibration", "snr_06_pump_discharge_pressure",
     "snr_07_pump_lube_oil_supply_temp", "snr_08_motor_casing_vibration", "snr_09_motor_phase_voltage", 'failure_cause_v2']]

corr = corr_df.corr()
# Extract correlations with the target variable:
correlation_with_target = corr[['failure_cause_v2']].drop(index='failure_cause_v2').sort_values(by='failure_cause_v2', ascending=False)

# Visualize the correlation with the target variable using a heatmap:
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_with_target, annot=True, fmt=".2f", cmap="RdYlGn", cbar=True, linewidths=2)
plt.title('Correlation of Features with Failure Cause', fontsize=12)
plt.xlabel('Failure Cause', fontsize=8)
plt.ylabel('Features', fontsize=8)
plt.xticks(rotation=45, ha='right')
plt.xticks(ticks=[], labels=[], rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

plt.figure(figsize=(26, 10))
sns.heatmap(corr, linewidths=4, annot=True, fmt=".2f", cmap="RdYlGn")
plt.title('Pearson Correlation Heatmap', fontsize=20)
plt.xlabel('Features', fontsize=15)
plt.ylabel('Features', fontsize=15)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

"""## **Selecting the features**<a id='Selecting_the_features'></a>
[Contents](#Contents)

Now that we have our cleaned dataset, it's time to set up our features (X) and target variable (y) for modeling.



"""

# let's define feature set (X) and target variable (y):
selected_features = ["day", "month", "hour",
     "snr_01_motor_phase_current", "snr_02_motor_shaft_power", "snr_03_motor_speed",
     "snr_04_pump_impeller_speed", "snr_05_pump_casing_vibration", "snr_06_pump_discharge_pressure",
     "snr_07_pump_lube_oil_supply_temp", "snr_08_motor_casing_vibration", "snr_09_motor_phase_voltage"]



X = df_failure[selected_features]

y = df_failure["failure_cause_v2"]

X.head()

y.head()

# Quick check:
print(X.shape, y.shape)

print(y.value_counts(normalize=True) * 100)

"""## **Splitting the Data for Training and Testing**<a id='Splitting_Data'></a>
[Contents](#Contents)

Now that we have our features (X) and target variable (y) set up, the next step is to split the data into training and testing sets.

Since this is time-series data, we need to be careful with how we split it. Instead of a random split, we'll preserve the order of time to avoid data leakage. We'll select a continuous chunk for training and another for testing to make the model more realistic.
"""

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                  test_size=0.5,
                                                 random_state=1981,
                                                 stratify=y  # Ensures classes are balanced
)


# Check split sizes:
print("Training set size:", X_train.shape, y_train.shape)
print("Testing set size:", X_test.shape, y_test.shape)

"""## **Scaling the Features**<a id='Scaling_Features'></a>
[Contents](#Contents)

Since our dataset has different types of sensor readings (current, power, speed, pressure), their values vary widely. Some sensors might have values in the hundreds, while others are in decimals. This can make it harder for machine learning models to perform well.

To fix this, we'll standardize the data using StandardScaler, which transforms all features to have zero mean and unit variance.
"""

# Initialize the scaler:
scaler = StandardScaler()

# Fit and transform the training data:
X_train_scaled = scaler.fit_transform(X_train)

# Transform the test data using the same scaler:
X_test_scaled = scaler.transform(X_test)

# Quick check on the scaled data:
print("Scaled training data shape:", X_train_scaled.shape)
print("Scaled testing data shape:", X_test_scaled.shape)

"""## **Training Machine Learning Model**<a id='modeling'></a>
[Contents](#Contents)

With the data cleaned, split, and scaled, we're ready to train classification models to predict failure categories. We'll start with a popular model like Random Forest Classifier and then based on the result and time we can test other models.
"""

model = RandomForestClassifier(n_estimators=100, random_state=223)

# Train the model:
model.fit(X_train_scaled, y_train)

"""## **Model Evaluation**<a id='Model_Evaluation'></a>
[Contents](#Contents)
"""

# Make predictions:
model_predictions = model.predict(X_test_scaled)

# Compute accuracy:
accuracy = accuracy_score(y_test, model_predictions)
print("\nModel Accuracy:", round(accuracy * 100, 2), "%")

# Generate confusion matrix:
cm = confusion_matrix(y_test, model_predictions)

# Define class labels
class_labels = ["Class 0", "Class 1", "Class 2", "Class 3", "Class 4", "Class 5"]

# Create a DataFrame for visualization
df_cm = pd.DataFrame(cm, index=class_labels, columns=[f"Predicted {label}" for label in class_labels])

# Plot confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(df_cm, annot=True, fmt='d', cmap="Blues", linewidths=0.5)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Random Forest")
plt.show()

"""**Summary Highlights:**

* Model: we trained a Random Forest Classifier with 100 trees, keeping it stable with random_state=223.
* Accuracy: Solid 99.87%
* Confusion Matrix: Mostly spot-on predictions, but a few misclassifications here and there.

## **Features Importance**<a id='Features_Importance'></a>
[Contents](#Contents)
"""

# Let's Extract feature importance from trained RandomForest model:
importances = model.feature_importances_

# Then, let's create a DataFrame to match feature names with importance values:
feature_importance_df = pd.DataFrame({'Feature': selected_features, 'Importance': importances})

# Sorting:
feature_importance_df = feature_importance_df.sort_values(by="Importance", ascending=False)

# Plot the feature importance using Seaborn:
plt.figure(figsize=(10, 6))
sns.barplot(x=feature_importance_df["Importance"], y=feature_importance_df["Feature"], palette="Blues_r")
plt.xlabel("Feature Importance Score")
plt.ylabel("Features")
plt.title("Feature Importance in Random Forest Model")
plt.show()

# Ensure df_failure contains only the subset of df used for predictions:
df_failure = df.loc[X_test.index].copy()

# Store predicted labels:
df_failure["Predicted_Label"] = model_predictions

# Get predicted probabilities
probabilities = model.predict_proba(X_test)

# Convert probabilities into a DataFrame with proper column names:
prob_df = pd.DataFrame(probabilities,
                       columns=[f"Prob_Class_{i}" for i in range(probabilities.shape[1])],
                       index=X_test.index)  # Ensure index alignment

# Merge probability columns into df_failure
df_failure = df_failure.join(prob_df):

# Now, update the original dataset df with the new predictions and probabilities:
df.loc[df_failure.index, df_failure.columns] = df_failure

# Display first few rows to verify:
df.head()

"""## **Saving the prediction dataset**<a id='Saving_prediction'></a>
[Contents](#Contents)
"""

# Alright, let's save the DataFrame as a CSV file:
df.to_csv('/content/drive/My Drive/Colab Notebooks/failure_cause_predictions.csv', index=False)

# Here's, let's call our helper function "summarize_df" to get more overall look about the columns in the dataset:
summarize_df(df)