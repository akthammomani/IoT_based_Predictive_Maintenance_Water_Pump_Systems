# -*- coding: utf-8 -*-
"""Data_Wrangling_Pre_Processing_Notebook_IoT_v1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rDMo9zD5nGcG123oGfqrOxcws5qcUVRx

# **Data Wrangling & Pre-processing: Predictive Maintenance of Water Pump Systems**

* **Group 9:** Aktham Almomani
* **Course:** Data Analytics and Internet of Things (AAI-530-04)/ University Of San Diego
* **Semester:** Spring 2025

<center>
    <img src="https://github.com/akthammomani/AI_powered_heart_disease_risk_assessment_app/assets/67468718/2cab2215-ce7f-4951-a43a-02b88a5b9fa9" alt="wrnagling">
</center>

## **Contents**<a is='Contents'></a>
* [Introduction](#Introduction)
* [Dataset](#Dataset)
* [Setup and preliminaries](#Setup_and_preliminaries)
  * [Import Libraries](#Import_libraries)
  * [Helper functions](#Helper_Functions)
* [Importing the dataset](#Importing_the_dataset)
* [Dataset Cleaning](#Dataset_Cleaning)
  * [Sensor columns Renaming](#Sensor_columns_Renaming)
  * [timestamp datatype](#timestamp_datatype)
  * [Missing Data](#missing_data)
  * [Selecting Key Sensors for Machine Health Monitoring](#Selecting_Sensors)
  * [Renaming Selected Sensors for Clarity](#Renaming_Sensors)
  * [Original Machine Status Field](#Original_Machine_Status)
* [Features Engineering](#Features_Engineering)
  * [New Machine Status Field](#New_Machine_Status)
  * [Machine Health Flags](#Machine_Health_Flags)
  * [Failure Category](#Failure_Category)
  * [Failure Cause](#Failure_Cause)
  * [Affected Sensors List](#Affected_Sensors_List)
* [Saving the cleaned dataframe](#Saving_the_cleaned_dataframe)

## **Introduction**<a id='Introduction'></a>
[Contents](#Contents)

In this notebook, we're getting our dataset in shape before any analysis. The focus is on **data wrangling and preprocessing** to make sure everything is clean, structured, and ready for insights.  

**What We'll Be Doing**:  

* **Cleaning the data**: Making sure everything is formatted correctly, handling missing values, and keeping only useful sensors.  

* **Preprocessing for analysis**: Converting timestamps, renaming columns, and standardizing sensor readings.  

* **Feature engineering**: Building smart features like **machine health flags, failure categories, and failure causes** to help us detect issues faster.  

By the end, we'll have a **clean, structured dataset** that makes it easy to track and predict the health of our machine

## **Dataset**<a id='Dataset'></a>
[Contents](#Contents)

The dataset we'll be using is called "Pump Sensor Data" and is available on [Kaggle](https://www.kaggle.com/datasets/nphantawee/pump-sensor-data/data). This dataset provides real-world IoT sensor data collected from water pump systems.

**Data Collection**: The data was gathered from sensors installed in water pump systems. These sensors monitor a wide range of parameters such as motor and pump vibrations, frequencies, power usage, and bearing temperatures. The collection process spans over time, capturing changes in system behavior and helping identify operational issues or equipment failures.

**Dataset Overview**:
* Number of Observations: The dataset contains 220,320 entries.
* Number of Variables: There are 55 columns, including 52 numerical variables (sensor readings), one timestamp column, and one categorical column representing machine status

## **Setup and preliminaries**<a id='Setup_and_preliminaries'></a>
[Contents](#Contents)

### **Import libraries**<a id='Import_libraries'></a>
[Contents](#Contents)
"""

#Let's import the necessary packages:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.ensemble import IsolationForest

# let's run below to customize notebook display:
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.max_colwidth', 4000) # 100 means 100 characters in the col

# format floating-point numbers to 2 decimal places:
pd.set_option('float_format', '{:.2f}'.format)

"""### **Helper functions**<a id='Helper_Functions'></a>
[Contents](#Contents)
"""

def summarize_df(df):
    """
    Generate a summary DataFrame for an input DataFrame:
      - 'unique_count': No. unique values in each column.
      - 'data_types': Data types of each column.
      - 'missing_counts': No. of missing (NaN) values in each column.
      - 'missing_percentage': Percentage of missing values in each column.
    """
    # No. of unique values for each column:
    unique_counts = df.nunique()
    # Data types of each column:
    data_types = df.dtypes
    # No. of missing (NaN) values in each column:
    missing_counts = df.isnull().sum()
    # Percentage of missing values in each column:
    missing_percentage = 100 * df.isnull().mean()
    # Concatenate the above metrics:
    summary_df = pd.concat([unique_counts, data_types, missing_counts, missing_percentage], axis=1)
    # Rename the columns for better readibility:
    summary_df.columns = ['unique_count', 'data_types', 'missing_counts', 'missing_percentage']
    # Return summary df:
    return summary_df
#-----------------------------------------------------------------------------------------------------------------#
def value_counts_with_percentage(df, column_name):
    # Calculate value counts:
    counts = df[column_name].value_counts(dropna=False)

    # Calculate percentages:
    percentages = df[column_name].value_counts(dropna=False, normalize=True) * 100

    # Combine into a DataFrame and reset index:
    result = pd.DataFrame({
        'Count': counts,
        'Percentage': percentages
    }).rename_axis(None).reset_index()

    # Rename the first column to match the original column name:
    result.rename(columns={'index': column_name}, inplace=True)

    return result
#-----------------------------------------------------------------------------------------------------------------#
def dynamic_impute(df, threshold=0.1):
    """
    Impute missing values dynamically based on the gap between mean and median.
    """
    # Get sensor columns:
    sensor_columns = [col for col in df.columns if col.startswith('snr')]

    for col in sensor_columns:
        mean_val = df[col].mean()
        median_val = df[col].median()

        # Calculate percentage difference:
        diff = abs(mean_val - median_val) / mean_val

        # Imputation decision:
        if diff <= threshold:
            # Mean and median are close, use the mean:
            df[col] = df[col].fillna(mean_val)
            print(f"Imputed '{col}' using mean (diff = {diff:.2%})")
        elif median_val > mean_val:
            # Median is significantly higher, use the median:
            df[col] = df[col].fillna(median_val)
            print(f"Imputed '{col}' using median because median > mean (diff = {diff:.2%})")
        else:
            # Use median for any other skewed cases:
            df[col] = df[col].fillna(median_val)
            print(f"Imputed '{col}' using median (diff = {diff:.2%})")

    return df

"""## **Importing the dataset**<a id='Importing_the_dataset'></a>
[Contents](#Contents)

Since I'm using google colab, let's mount the driver:
"""

from google.colab import drive
drive.mount('/content/drive')

#First, let's load the main dataset sensor: (https://www.kaggle.com/datasets/nphantawee/pump-sensor-data/data)
df = pd.read_csv(
    '/content/drive/My Drive/Colab Notebooks/sensor.csv',
    index_col=0
)

#now, let's look at the shape of df:
shape = df.shape
print("Number of rows:", shape[0], "\nNumber of columns:", shape[1])

# Now, let's look at the top 5 rows of the df:
df.head()

"""## **Dataset Cleaning**<a id='Dataset_Cleaning'></a>
[Contents](#Contents)

* First, let's make sure no white space.
* **Sensor column names**: The names are just numbers (`sensor_01`, `sensor_02`, etc.). We should check the dataset documentation to see if better names are available and rename the sensors accordingly.
* **`timestamp` column**: Right now, it's an object (string). We'll convert it to `datetime` to make it easier for time-based analysis.

* **Missing data**: All sensor columns have some missing values, so we need to handle that.
  * **`sensor_15`** is completely empty (100% missing). It's useless, so it should go.
  * **`sensor_50`** has a lot of missing values (about 35%). We'll probably drop it too. Before deciding to drop it, let's first check its purpose or role in the dataset.
* **Selecting key sensors**: Not all sensors are equally important. We will focus on the most relevant ones for machine health monitoring.
* **Renaming selected sensors**: Clearer names make it easier to interpret the data, so we will rename the selected sensors based on their function.  
* **Dropping the original machine status**: The provided machine status isn't always reliable, so we'll be building a more accurate version. Once that's in place, we'll remove the original column.

"""

# Alright, now, let's make sure no white space in the dataset, so if any will be replaced with NAN:
df.replace("", np.nan, inplace=True)

# Here's, let's call our helper function "summarize_df" to get more overall look about the columns in the dataset:
summarize_df(df)

# alright, now let's look if there's any duplicate values:
num_duplicates = df['timestamp'].duplicated().sum()
print(f"Number of duplicate rows based on 'timestamp': {num_duplicates}")

"""Alright, this is good. No duplicates in the dataset!

### **Sensor columns Renaming**<a id='Sensor_columns_Renaming'></a>
[Contents](#Contents)

Before renaming the columns, let's cross-check the names provided [here](https://www.kaggle.com/datasets/nphantawee/pump-sensor-data/discussion/131429) with the statistics of each sensor to ensure they align and provide better clarity:

* SENSOR_00 - Motor Casing Vibration
* SENSOR_01 - Motor Frequency A
* SENSOR_02 - Motor Frequency B
* SENSOR_03 - Motor Frequency C
* SENSOR_04 - Motor Speed
* SENSOR_05 - Motor Current
* SENSOR_06 - Motor Active Power
* SENSOR_07 - Motor Apparent Power
* SENSOR_08 - Motor Reactive Power
* SENSOR_09 - Motor Shaft Power  
* SENSOR_10 - Motor Phase Current A
* SENSOR_11 - Motor Phase Current B
* SENSOR_12 - Motor Phase Current C
* SENSOR_13 - Motor Coupling Vibration
* SENSOR_14 - Motor Phase Voltage AB
* SENSOR_16 - Motor Phase Voltage BC
* SENSOR_17 - Motor Phase Voltage CA
* SENSOR_18 - Pump Casing Vibration
* SENSOR_19 - Pump Stage 1 Impeller Speed
* SENSOR_20 - Pump Stage 1 Impeller Speed
* SENSOR_21 - Pump Stage 1 Impeller Speed
* SENSOR_22 - Pump Stage 1 Impeller Speed
* SENSOR_23 - Pump Stage 1 Impeller Speed
* SENSOR_24 - Pump Stage 1 Impeller Speed
* SENSOR_25 - Pump Stage 2 Impeller Speed
* SENSOR_26 - Pump Stage 2 Impeller Speed
* SENSOR_27 - Pump Stage 2 Impeller Speed
* SENSOR_28 - Pump Stage 2 Impeller Speed
* SENSOR_29 - Pump Stage 2 Impeller Speed
* SENSOR_30 - Pump Stage 2 Impeller Speed
* SENSOR_31 - Pump Stage 2 Impeller Speed
* SENSOR_32 - Pump Stage 2 Impeller Speed
* SENSOR_33 - Pump Stage 2 Impeller Speed
* SENSOR_34 - Pump Inlet Flow
* SENSOR_35 - Pump Discharge Flow
* SENSOR_36 - Pump UNKNOWN
* SENSOR_37 - Pump Lube Oil Overhead Reservoir Level
* SENSOR_38 - Pump Lube Oil Return Temp
* SENSOR_39 - Pump Lube Oil Supply Temp
* SENSOR_40 - Pump Thrust Bearing Active Temp
* SENSOR_41 - Motor Non Drive End Radial Bearing Temp 1
* SENSOR_42 - Motor Non Drive End Radial Bearing Temp 2
* SENSOR_43 - Pump Thrust Bearing Inactive Temp
* SENSOR_44 - Pump Drive End Radial Bearing Temp 1
* SENSOR_45 - Pump non Drive End Radial Bearing Temp 1
* SENSOR_46 - Pump Non Drive End Radial Bearing Temp 2
* SENSOR_47 - Pump Drive End Radial Bearing Temp 2
* SENSOR_48 - Pump Inlet Pressure
* SENSOR_49 - Pump Temp Unknown
* SENSOR_50 - Pump Discharge Pressure 1
* SENSOR_51 - Pump Discharge Pressure 2
"""

df.describe()

"""**Observations and Adjustments:**

Based on the data:

* **sensor_00**:
  * **Proposed Name**: Motor Casing Vibration.
  * **Observation**: Small mean and low range suggest vibration. Name is valid.

* **sensor_01, sensor_02, sensor_03**:
  * **Proposed Name**: Motor Frequency A, B, C.
  * **Observation**: Values stay under 60, aligning with frequency in Hz. Names are valid.

* **sensor_04**:
  * **Proposed Name**: Motor Speed (RPM).
  * **Observation**: High values (~590 mean, max 800) fit RPM. Name is valid.

* **sensor_05**:
  * **Proposed Name**: Motor Current.
  * **Observation**: Moderate values (~73 mean, max 99). Likely a current sensor. Name is valid.

* **sensor_06 to sensor_09**:
  * **Proposed Names**: Active, Apparent, Reactive, Shaft Power.
  * **Observation**: Low but consistent values (13 - 15 range). Names are valid.

* **sensor_10 to sensor_12**:
  * **Proposed Names**: Phase Currents (A, B, C).
  * **Observation**: Moderate values (~41 mean, max 76). These align with electrical currents. Names are valid.

* **sensor_13**:
  * **Proposed Name**: Motor Coupling Vibration.
  * **Observation**: Low values (~7 mean, max 31). Fits vibration readings. Name is valid.

* **sensor_14 to sensor_17**:
  * **Proposed Names**: Motor Phase Voltages (AB, BC, CA).
  * **Observation**: High values (~376 - 421 mean, max ~600+). These fit voltage readings. Names are valid.

* **sensor_18**:
  * **Proposed Name**: Pump Casing Vibration.
  * **Observation**: Low range (~2 mean, max ~4). Matches vibration. Name is valid.

* **sensor_19 to sensor_33**:
  * **Proposed Names**: Pump Impeller Speeds (Stage 1 & 2).
  * **Observation**: High and consistent values (~500 - 900 mean, max ~1200). Fits speed sensors. Names are valid.

* **sensor_34 and sensor_35**:
  * **Proposed Names**: Pump Inlet and Discharge Flow.
  * **Observation**: Values (~234 and ~427 mean, max ~500 - 700). Likely flow rates. Names are valid.

* **sensor_36**:
  * **Proposed Name**: Pump UNKNOWN.
  * **Observation**: High range (~593 mean, max ~984). Could be flow or pressure. Let's name this one as 'Pump Auxiliary Flow or Pressure'

* **sensor_37**:
  * **Proposed Name**: Pump Lube Oil Overhead Reservoir Level.
  * **Observation**: Low mean (~60), consistent with a level sensor. Name is valid.

* **sensor_38 to sensor_44**:
  * **Proposed Names**: Pump Lube Oil and Bearing Temperatures.
  * **Observation**: Values (~35 - 68 mean, max ~400). Names are valid.

* **sensor_45 to sensor_49**:
  * **Proposed Names**: Pump Radial Bearing Temperatures.
  * **Observation**: Values (~42 - 57 mean, max ~464). Names are valid.

* **sensor_50 and sensor_51**:
  * **Proposed Names**: Pump Discharge Pressures (1 & 2).
  * **Observation**: High values (~183 - 202 mean, max ~1000). Names are valid.

"""

sensor_name_mapping = {
    'sensor_00': 'snr_00_motor_casing_vibration',
    'sensor_01': 'snr_01_motor_frequency_a',
    'sensor_02': 'snr_02_motor_frequency_b',
    'sensor_03': 'snr_03_motor_frequency_c',
    'sensor_04': 'snr_04_motor_speed',
    'sensor_05': 'snr_05_motor_current',
    'sensor_06': 'snr_06_motor_active_power',
    'sensor_07': 'snr_07_motor_apparent_power',
    'sensor_08': 'snr_08_motor_reactive_power',
    'sensor_09': 'snr_09_motor_shaft_power',
    'sensor_10': 'snr_10_motor_phase_current_a',
    'sensor_11': 'snr_11_motor_phase_current_b',
    'sensor_12': 'snr_12_motor_phase_current_c',
    'sensor_13': 'snr_13_motor_coupling_vibration',
    'sensor_14': 'snr_14_motor_phase_voltage_ab',
    'sensor_15': 'snr_15_PH',  # missed 100% will be dropped
    'sensor_16': 'snr_16_motor_phase_voltage_bc',
    'sensor_17': 'snr_17_motor_phase_voltage_ca',
    'sensor_18': 'snr_18_pump_casing_vibration',
    'sensor_19': 'snr_19_pump_stage_1_impeller_speed',
    'sensor_20': 'snr_20_pump_stage_1_impeller_speed',
    'sensor_21': 'snr_21_pump_stage_1_impeller_speed',
    'sensor_22': 'snr_22_pump_stage_1_impeller_speed',
    'sensor_23': 'snr_23_pump_stage_1_impeller_speed',
    'sensor_24': 'snr_24_pump_stage_1_impeller_speed',
    'sensor_25': 'snr_25_pump_stage_2_impeller_speed',
    'sensor_26': 'snr_26_pump_stage_2_impeller_speed',
    'sensor_27': 'snr_27_pump_stage_2_impeller_speed',
    'sensor_28': 'snr_28_pump_stage_2_impeller_speed',
    'sensor_29': 'snr_29_pump_stage_2_impeller_speed',
    'sensor_30': 'snr_30_pump_stage_2_impeller_speed',
    'sensor_31': 'snr_31_pump_stage_2_impeller_speed',
    'sensor_32': 'snr_32_pump_stage_2_impeller_speed',
    'sensor_33': 'snr_33_pump_stage_2_impeller_speed',
    'sensor_34': 'snr_34_pump_inlet_flow',
    'sensor_35': 'snr_35_pump_discharge_flow',
    'sensor_36': 'snr_36_pump_auxiliary_flow_or_pressure',
    'sensor_37': 'snr_37_pump_lube_oil_overhead_reservoir_level',
    'sensor_38': 'snr_38_pump_lube_oil_return_temp',
    'sensor_39': 'snr_39_pump_lube_oil_supply_temp',
    'sensor_40': 'snr_40_pump_thrust_bearing_active_temp',
    'sensor_41': 'snr_41_motor_non_drive_end_radial_bearing_temp_1',
    'sensor_42': 'snr_42_motor_non_drive_end_radial_bearing_temp_2',
    'sensor_43': 'snr_43_pump_thrust_bearing_inactive_temp',
    'sensor_44': 'snr_44_pump_drive_end_radial_bearing_temp_1',
    'sensor_45': 'snr_45_pump_non_drive_end_radial_bearing_temp_1',
    'sensor_46': 'snr_46_pump_non_drive_end_radial_bearing_temp_2',
    'sensor_47': 'snr_47_pump_drive_end_radial_bearing_temp_2',
    'sensor_48': 'snr_48_pump_inlet_pressure',
    'sensor_49': 'snr_49_pump_temp_unknown',
    'sensor_50': 'snr_50_pump_discharge_pressure_1',# since i have 2 sensors with pressure "sensor_50 & sensor_51" I will be dropping sensor_50 since it has ~35% missing data
    'sensor_51': 'snr_51_pump_discharge_pressure'
}

# Alright, now let's apply the mapping
df.rename(columns=sensor_name_mapping, inplace=True)

df.head()

"""Ok, now the column names are much more meaningful and easier to understand.

### **timestamp datatype**<a id='timestamp_datatype'></a>
[Contents](#Contents)
"""

# Alright, let's fix up 'timestamp' to be datetime:
df['timestamp']=pd.to_datetime(df['timestamp'])

# Here's, let's call again our helper function "summarize_df" to get more overall look about the columns in the dataset:
summarize_df(df)

"""### **Missing Data**<a id='missing_data'></a>
[Contents](#Contents)

* **Missing data**: All sensor columns have some missing values, so we need to handle that. Also:
  * **`sensor_15`** is completely empty (100% missing). It's useless, so it should go.
  * **`sensor_50`** has a lot of missing values (~35%). We'll probably drop it too.
"""

# Alright, let's drop 'snr_15_PH' and 'snr_50_pump_discharge_pressure_1' since we don't need them:
df = df.drop(columns=['snr_15_PH', 'snr_50_pump_discharge_pressure_1'], errors='ignore')

# Alright, let's take a look at the stats to decide, mean or median imputation:
df.describe()

# Alright, let's focus only on the sensor columns and check the mean vs. median difference:
sensor_columns = [col for col in df.columns if col.startswith('snr')]

for col in sensor_columns:
    mean_val = round(df[col].mean(), 2)
    median_val = round(df[col].median(), 2)
    print(f"{col}: Mean = {mean_val}, Median = {median_val}")

# Alright, let's dynamically impute the missing values: if the difference between mean and median for sensor columns is high then use median if it's low then use mean:
df = dynamic_impute(df, threshold=0.1)

# Here's, let's call again our helper function "summarize_df" to get more overall look about the columns in the dataset:
summarize_df(df)

#now, let's look at the shape of df:
shape = df.shape
print("Number of rows:", shape[0], "\nNumber of columns:", shape[1])

"""Awesome, there're no more missing data and also we managed to keep dataset size as is."""

df.head()

df.describe()

"""Alright, the dataset spans a 5-month period from April 1, 2018, to August 31, 2018.

### **Selecting Key Sensors for Machine Health Monitoring**<a id='Selecting_Sensors'></a>
[Contents](#Contents)

In industrial applications, sensor data is crucial for monitoring machine health and predicting failures. However, with dozens of available sensors, selecting the most relevant ones is essential for accurate classification while avoiding noise from less impactful signals.

To ensure an optimal selection, we will use a statistical significance approach combined with domain expertise to identify the top 10 sensors that have the most influence on machine status.
"""

# First, let's identify sensor columns:
sensor_columns = [col for col in df.columns if "snr_" in col]

# Compute statistics for each sensor and let's Transpose for better readability:
sensor_stats = df[sensor_columns].describe().T

# Calculate additional metrics:
sensor_stats["range"] = sensor_stats["max"] - sensor_stats["min"]
sensor_stats["std_ratio"] = sensor_stats["std"] / sensor_stats["mean"]

# Display sensor statistics:
sensor_stats_sorted = sensor_stats.sort_values(by="std_ratio", ascending=False)
sensor_stats_sorted

"""Now let's visualize all sensors:"""

def plot_all_sensors(df, sensors=None, title="All Sensor Readings Over Time"):
    """
    Plots all sensor readings over time as subplots with properly aligned titles
    """

    if sensors is None:
        sensors = [col for col in df.columns if col.startswith("snr_")]  # Auto-detect sensors

    num_sensors = len(sensors)
    fig, axes = plt.subplots(num_sensors, 1, figsize=(14, num_sensors), sharex=True)

    if num_sensors == 1:
        axes = [axes]

    for i, sensor in enumerate(sensors):
        axes[i].plot(df["timestamp"], df[sensor], linewidth=1)
        axes[i].set_title(sensor, fontsize=10, fontweight='bold', pad=8)
        axes[i].grid(True)

    # Formatting:
    axes[-1].set_xlabel("Timestamp", fontsize=12)
    fig.suptitle(title, fontsize=14, fontweight='bold', y=0.98)
    plt.xticks(rotation=45)
    plt.tight_layout(rect=[0, 0, 1, 0.98])
    plt.show()

# Plot all sensors in the dataset:
plot_all_sensors(df)

"""**How we picked the Sensors that matter**  

When working with machine data, we don't want to include **every single sensor** that would be inefficient and redundant. Instead, we took a smart, data-driven approach to picking the ones that actually matter.  

**What we considered**  

* **Statistical Significance**  
Not all sensors provide useful signals. Some barely change, while others fluctuate wildly. We checked things like:  
  * **Standard deviation & range**: If a sensor never changes, it's not helpful.  
  * **Variability**: Sensors with meaningful variations over time are key for detecting machine health changes.  

* **Industrial knowledge & vommon sense**  
Some sensors are **more critical** for understanding how the machine is doing. From experience, we know:  
  * **Vibration sensors**: Catch early signs of failure.  
  * **Motor current, power, and speed**: Reflect performance & efficiency.  
  * **Temperature sensors**: Help spot overheating issues.  

* **Avoiding redundant sensors**  
Machines have **a lot** of sensors measuring similar things. For example, if we have 10 temperature sensors but they all show the **same trend**, we don't need all of them. We compared sensor behavior and **kept the most representative ones**.  

* **Visualizing everything**  
We plotted **all sensor data over time** to see which ones actually showed useful trends. If a sensor barely moved or looked exactly like another one, we dropped it.  

**The 10 Sensors We're Keeping**: After all that, here are the **top 10 sensors** that give us the best picture of machine health:  

  * **snr_12_motor_phase_current_c**: Motor current phase C  
  * **snr_09_motor_shaft_power**: Motor shaft power  
  * **snr_04_motor_speed**: Motor speed  
  * **snr_19_pump_stage_1_impeller_speed**: Pump impeller speed  
  * **snr_18_pump_casing_vibration**: Pump casing vibration  
  * **snr_51_pump_discharge_pressure**: Pump discharge pressure  
  * **snr_39_pump_lube_oil_supply_temp**: Lube oil supply temperature  
  * **snr_18_pump_casing_vibration**: (Yes, again! Key vibration metric)  
  * **snr_00_motor_casing_vibration**: Motor casing vibration  
  * **snr_17_motor_phase_voltage_ca**: Motor voltage phase CA  

These sensors cover **electrical, mechanical, and thermal** aspects of the machine, giving us a **well-rounded** view without extra noise.  


"""

# Define the selected sensor columns:
selected_sensors = [
    "snr_12_motor_phase_current_c",
    "snr_09_motor_shaft_power",
    "snr_04_motor_speed",
    "snr_19_pump_stage_1_impeller_speed",
    "snr_18_pump_casing_vibration",
    "snr_51_pump_discharge_pressure",
    "snr_39_pump_lube_oil_supply_temp",
    "snr_00_motor_casing_vibration",
    "snr_17_motor_phase_voltage_ca"
]

# Keep only the selected sensors + timestamp + existing machine status:
df = df[["timestamp", "machine_status"] + selected_sensors]
df.head()

"""### **Renaming Selected Sensors for Clarity**<a id='Renaming_Sensors'></a>
[Contents](#Contents)

Our sensor names were a bit long and technical, so we're giving them **cleaner, more intuitive names** while keeping their original meanings intact.  

| **Old Sensor Name**                          | **New Sensor Name**                | **Description** |
|----------------------------------------------|------------------------------------|----------------|
| `snr_12_motor_phase_current_c`              | `snr_01_motor_phase_current`       | Measures the electrical current in the motor phase. |
| `snr_09_motor_shaft_power`                  | `snr_02_motor_shaft_power`         | Indicates the power output of the motor shaft. |
| `snr_04_motor_speed`                         | `snr_03_motor_speed`               | Monitors the speed of the motor. |
| `snr_19_pump_stage_1_impeller_speed`        | `snr_04_pump_impeller_speed`       | Tracks the speed of the pump's impeller. |
| `snr_18_pump_casing_vibration`              | `snr_05_pump_casing_vibration`     | Measures vibrations in the pump casing. |
| `snr_51_pump_discharge_pressure`            | `snr_06_pump_discharge_pressure`   | Reads the pressure at the pump's discharge point. |
| `snr_39_pump_lube_oil_supply_temp`          | `snr_07_pump_lube_oil_supply_temp` | Monitors the temperature of lubricating oil. |
| `snr_00_motor_casing_vibration`             | `snr_08_motor_casing_vibration`    | Tracks vibration levels in the motor casing. |
| `snr_17_motor_phase_voltage_ca`             | `snr_09_motor_phase_voltage`       | Measures voltage across motor phases. |

Now, let's apply these new names to our dataset.

"""

# Define mapping of old sensor names to new names:
sensor_rename_map = {
    "snr_12_motor_phase_current_c": "snr_01_motor_phase_current",
    "snr_09_motor_shaft_power": "snr_02_motor_shaft_power",
    "snr_04_motor_speed": "snr_03_motor_speed",
    "snr_19_pump_stage_1_impeller_speed": "snr_04_pump_impeller_speed",
    "snr_18_pump_casing_vibration": "snr_05_pump_casing_vibration",
    "snr_51_pump_discharge_pressure": "snr_06_pump_discharge_pressure",
    "snr_39_pump_lube_oil_supply_temp": "snr_07_pump_lube_oil_supply_temp",
    "snr_00_motor_casing_vibration": "snr_08_motor_casing_vibration",
    "snr_17_motor_phase_voltage_ca": "snr_09_motor_phase_voltage"
}

# Rename the columns in the filtered dataframe:
df = df.rename(columns=sensor_rename_map)

df.head()

#now, let's look at the shape of df:
shape = df.shape
print("Number of rows:", shape[0], "\nNumber of columns:", shape[1])

# Here's, let's call again our helper function "summarize_df" to get more overall look about the columns in the dataset:
summarize_df(df)

"""### **Original Machine Status Field**<a id='Original_Machine_Status'></a>
[Contents](#Contents)

Before we rely on the dataset's **machine status** field, we need to ask: **"Is it actually accurate?"** If the labels are incorrect or inconsistent, any analysis or model we build will be unreliable.  

**How we'll investigate**  

We'll compare the **existing machine status** with what the selected sensors tell us. Specifically, we will:  
* **Check distribution**: Are the labels balanced, or do they seem off?  
* **Plot trends**: Does the machine status match sensor readings over time?  
* **Find contradictions**: Are there cases where the machine is labeled as "Normal" while critical sensors show major anomalies?  

If we see clear **mismatches**, we'll **redefine machine status** using a **data-driven** approach based on the 10 key sensors we selected earlier.  
"""

value_counts_with_percentage(df, 'machine_status')

"""**Summary Highlights**


* **Way Too Many Normal Cases**: A whopping 93.43 percent of the data is labeled as Normal. That is suspicious. Machines do not just run perfectly all the time.   

* **Barely Any Broken Cases**: Only 7 instances 0.00 percent are marked as Broken. Either this machine is invincible or the dataset is not catching failures properly.  

* **Recovering Without Broken**: We have got 14477 cases of Recovering but only 7 Broken. How does something recover if it never breaks That is like healing from an injury you never had.  

The current machine status labels are off. If we rely on them any predictive model we build will be flawed. alright, let's try to fix it

let's verify the accuracy of the machine status!!
"""

# Select key sensors for comparison:
key_sensors = [
    "snr_01_motor_phase_current",
    "snr_02_motor_shaft_power",
    "snr_03_motor_speed",
    "snr_04_pump_impeller_speed",
    "snr_05_pump_casing_vibration"
]

# Ensure machine status is in a valid format:
df["machine_status"] = df["machine_status"].str.strip().str.upper()

# Create a numeric mapping for machine status:
status_mapping = {"NORMAL": 0, "RECOVERING": 1, "BROKEN": 2}
df["status_numeric"] = df["machine_status"].map(status_mapping)


# Define colors for each status:
color_map = df["machine_status"].map({"NORMAL": "green", "RECOVERING": "orange", "BROKEN": "red"})

# Plot machine status alongside key sensors:
fig, axes = plt.subplots(len(key_sensors) + 1, 1, figsize=(12, 8), sharex=True)

# Plot the machine status using scatter for better visibility:
axes[0].scatter(df["timestamp"], df["status_numeric"], c=color_map, label="Machine Status", alpha=0.7)
axes[0].set_title("Machine Status Over Time", fontsize=12)
axes[0].legend()
axes[0].set_yticks([0, 1, 2])
axes[0].set_yticklabels(["Normal", "Recovering", "Broken"])

# Plot sensor trends:
for i, sensor in enumerate(key_sensors):
    axes[i + 1].plot(df["timestamp"], df[sensor], label=sensor)
    axes[i + 1].set_title(sensor, fontsize=10)
    axes[i + 1].legend()

plt.xlabel("Time", fontsize=12)
plt.tight_layout()
plt.show()

"""**Summary Highlights:**

* The "Broken" and "Recovering" status points seem scattered and do not consistently align with sharp changes in sensor readings.
* Some significant drops or fluctuations in motor speed, shaft power, and impeller speed do not always result in a status change. The system is still marked as "Normal."
* Recovering events seem to appear in isolation rather than following a clear "Broken" phase, which raises questions about how the dataset originally labeled them.
"""

# Ok, so let's drop the original machine_status column:
df = df.drop(columns=["machine_status", "status_numeric"])

# Verify that the column is removed:
df.head()

"""## **Features Engineering**<a id='Features_Engineering'></a>
[Contents](#Contents)

We are not just looking at raw sensor data, we are building smart features to make sense of it all. Here is what we have  

* **New Machine Status**  
  Instead of relying on the dataset's status, which was not super accurate, we created a cleaned-up version that separates Normal from Abnormal based on sensor behavior.  

* **Machine Health Flags**  
  These binary flags help us spot issues instantly. If a sensor goes beyond a threshold, it flips to 1 for abnormal, making it super easy to track problems.  

* **Failure Category**  
  Not all failures are equal. We group them into Critical and Moderate so we can prioritize what matters most rather than getting lost in minor fluctuations.  

* **Failure Cause**  
  This digs even deeper, telling us where the problem is, such as rotational, vibration, electrical, and more. If multiple systems are acting up, we list them all.  

* **Affected Sensors List**  
  Instead of just saying there is an issue, we now track exactly which sensors detected abnormalities. This helps pinpoint what needs attention and speeds up troubleshooting.   

With these features, we are turning raw sensor data into real insights, ready for dashboards, predictive models, and smarter decisions.

### **New Machine Status Field**<a id='New_Machine_Status'></a>
[Contents](#Contents)

* **Normal:** The machine is operating as expected.
* **Abnormal:** Any deviation from normal, including early signs of degradation, severe faults, or recovering phases.

**Advantages**:

* Covers all failure types without needing complex classifications.
* Easy to implement for monitoring and automation.
* Captures early warnings while also including critical failures.
"""

# Selected sensors for classification:
selected_sensors = [
    "snr_01_motor_phase_current",
    "snr_02_motor_shaft_power",
    "snr_03_motor_speed",
    "snr_04_pump_impeller_speed",
    "snr_05_pump_casing_vibration",
    "snr_06_pump_discharge_pressure",
    "snr_07_pump_lube_oil_supply_temp",
    "snr_08_motor_casing_vibration",
    "snr_09_motor_phase_voltage"
]

# let's calculate dynamic thresholds:
def calculate_dynamic_thresholds(df, sensor_columns, threshold=2.5):
    """
    Computes upper and lower bounds for sensors based on mean and standard deviation.
    Any value beyond these bounds is considered abnormal.
    """
    sensor_thresholds = {}

    for col in sensor_columns:
        mean_val = df[col].mean()
        std_val = df[col].std()

        lower_bound = mean_val - (threshold * std_val)
        upper_bound = mean_val + (threshold * std_val)

        sensor_thresholds[col] = {"lower": lower_bound, "upper": upper_bound}

    return sensor_thresholds

# Compute thresholds using the selected sensors:
thresholds = calculate_dynamic_thresholds(df, selected_sensors, threshold=2.5)

# Function to classify machine status as Normal or Abnormal:
def classify_machine_status_abnormal(sensor_data, thresholds):
    """
    Classifies machine status into two categories: Normal vs Abnormal.
    A machine is considered Abnormal if any selected sensor deviates beyond its threshold.
    """
    for sensor, limits in thresholds.items():
        if sensor in sensor_data:
            if (sensor_data[sensor] < limits["lower"]) or (sensor_data[sensor] > limits["upper"]):
                return "Abnormal"  # If any sensor is outside the range will be marked Abnormal

    return "Normal"  # Otherwise, it's Normal

# Apply classification to each row in the dataset:
df["machine_status"] = df.apply(lambda row: classify_machine_status_abnormal(row.to_dict(), thresholds), axis=1)

# New simplified machine status:
value_counts_with_percentage(df, 'machine_status')

def plot_machine_status(df, selected_sensors):
    """
    Plots the machine status alongside selected sensor readings.
    - Normal: green at y=1
    - Abnormal: red at y=0
    """
    fig, axes = plt.subplots(len(selected_sensors) + 1, 1, figsize=(12, 10), sharex=True)

    # Separate Normal and Abnormal timestamps:
    normal_data = df[df["machine_status"] == "Normal"]
    abnormal_data = df[df["machine_status"] == "Abnormal"]

    # Plot machine status with separate y-values:
    axes[0].scatter(normal_data["timestamp"], [0] * len(normal_data), c="green", label="Normal", alpha=0.7)
    axes[0].scatter(abnormal_data["timestamp"], [1] * len(abnormal_data), c="red", label="Abnormal", alpha=0.7)
    axes[0].set_title("Machine Status Over Time")
    axes[0].set_yticks([0, 1])  # let's set clear labels for both Normal (1) and Abnormal (0)
    axes[0].legend()

    # Plot sensor data:
    for i, sensor in enumerate(selected_sensors):
        axes[i + 1].plot(df["timestamp"], df[sensor], label=sensor)
        axes[i + 1].set_title(sensor)

    plt.xlabel("Time")
    plt.tight_layout()
    plt.show()

# Alright, now let's run the visualization:
plot_machine_status(df, selected_sensors)

"""**Summary highlights:**

* We cleaned up the machine status to just **Normal (Green) vs Abnormal (Red)** and that will keep things simple and clear.  
* Status now **actually reflects** sensor trends, when things spike, we see it in the status.  
* **No more weird transitions** (like skipping steps or looping between states).

### **Machine Health Flags**<a id='Machine_Health_Flags'></a>
[Contents](#Contents)

When dealing with industrial machines, it's not just about knowing when something is completely broken—we also want to catch early warning signs. Instead of sifting through raw sensor data, we'll create clear yes/no flags to tell us when something is behaving abnormally.

* We'll use dynamic thresholds (based on statistical analysis) to set normal operating ranges for each sensor.
* If a sensor's value goes beyond this range, we flag it as 1 (Yes, abnormal).
* If it's within the expected limits, we flag it as 0 (No, normal).
"""

# First let's compute the dynamic thresholds:
sensor_columns = [
    "snr_01_motor_phase_current",
    "snr_02_motor_shaft_power",
    "snr_03_motor_speed",
    "snr_04_pump_impeller_speed",
    "snr_05_pump_casing_vibration",
    "snr_06_pump_discharge_pressure",
    "snr_07_pump_lube_oil_supply_temp",
    "snr_08_motor_casing_vibration",
    "snr_09_motor_phase_voltage"
]

thresholds = calculate_dynamic_thresholds(df, sensor_columns, threshold=2.5)

# Then let's generate flags based on thresholds:
for sensor in sensor_columns:
    flag_col = f"{sensor}_flag"
    lower, upper = thresholds[sensor]["lower"], thresholds[sensor]["upper"]

    # Flag as 1 if the sensor value is outside the dynamic range:
    df[flag_col] = ((df[sensor] < lower) | (df[sensor] > upper)).astype(int)

# Alright finally, let's view the new columns:
df.head()

# First let's visualize all flags (Abnromal & Normal):
flag_columns = [col for col in df.columns if col.endswith("_flag")]

# Calculate normal and abnormal percentages:
total_counts = df[flag_columns].count()
abnormal_counts = (df[flag_columns] == 1).sum()
normal_counts = total_counts - abnormal_counts

abnormal_percentages = (abnormal_counts / total_counts) * 100
normal_percentages = (normal_counts / total_counts) * 100

# Sort sensors by highest abnormal percentage:
sorted_indices = abnormal_percentages.sort_values(ascending=False).index
abnormal_percentages = abnormal_percentages[sorted_indices]
normal_percentages = normal_percentages[sorted_indices]

# Plot stacked horizontal bar chart:
fig, ax = plt.subplots(figsize=(12, 8))
bars1 = ax.barh(sorted_indices, normal_percentages, color="green", label="Normal (%)")
bars2 = ax.barh(sorted_indices, abnormal_percentages, left=normal_percentages, color="red", label="Abnormal (%)")

# Add percentage labels inside bars:
for bars, color in zip([bars1, bars2], ["white", "black"]):
    for bar in bars:
        width = bar.get_width()  # Get bar length
        if width > 5:
            ax.text(
                bar.get_x() + width / 2,  # Center text horizontally
                bar.get_y() + bar.get_height() / 2,  # Center text vertically
                f"{width:.1f}%",  # Format as percentage
                ha="center", va="center", fontsize=10, color=color
            )

# Formatting:
ax.set_xlabel("Percentage of Readings (%)", fontsize=12)
ax.set_title("Normal vs Abnormal Readings Across Sensors", fontsize=14)
ax.legend(loc="lower right")
plt.gca().invert_yaxis()
plt.show()

# For here, let's visualize all flags with abnormal status:
flag_columns = [col for col in df.columns if col.endswith("_flag")]

# Calculate abnormal percentages:
total_counts = df[flag_columns].count()
abnormal_counts = (df[flag_columns] == 1).sum()
abnormal_percentages = (abnormal_counts / total_counts) * 100

# Sort sensors by highest abnormal percentage:
sorted_indices = abnormal_percentages.sort_values(ascending=False).index
abnormal_percentages = abnormal_percentages[sorted_indices]

# Plot horizontal bar chart:
fig, ax = plt.subplots(figsize=(12, 8))
bars = ax.barh(sorted_indices, abnormal_percentages, color="red", label="Abnormal (%)")

# Add labels inside bars:
for bar in bars:
    width = bar.get_width()
    text_position = width + 0.5
    text_color = "white" if width > 10 else "black"

    ax.text(
        max(text_position, 1.2),
        bar.get_y() + bar.get_height() / 2,
        f"{width:.1f}%",
        va="center", ha="right", fontsize=10, color=text_color, fontweight="bold"
    )

# Formatting:
ax.set_xlabel("Percentage of Abnormal Readings (%)", fontsize=12)
ax.set_title("Abnormal Readings Percentage Across Sensors", fontsize=14)
ax.legend(loc="lower right")
plt.gca().invert_yaxis()
plt.show()

"""### **Failure Category**<a id='Failure_Category'></a>
[Contents](#Contents)  

To make failure analysis more practical, we're grouping failures into **two  broad categories** instead of dealing with too many individual sensor issues. This keeps things **clear and actionable** while still giving us useful insights.  

**Categories Details:**  
* **Critical Failure**: Major issues, highest failure rates (>=8%)  
* **Moderate Failure**: Early warning signs, mid-range failure rates (less than 8%)  

This approach helps us **track overall machine health** without getting lost in sensor-level noise.

"""

# First let's upper case machine_status:
df["machine_status"] = df["machine_status"].str.strip().str.upper()

# DThen let's define our logic for the failure categories based on abnormal percentages:
def categorize_failure(percentage):
    if percentage >= 8:
        return "critical_failure"
    else:
        return "moderate_failure"

# Get flag columns:
flag_columns = [col for col in df.columns if col.endswith("_flag")]

# Calculate abnormal percentages:
total_counts = df[flag_columns].count()
abnormal_counts = (df[flag_columns] == 1).sum()
abnormal_percentages = (abnormal_counts / total_counts) * 100

# Assign failure categories for each sensor:
failure_categories = abnormal_percentages.apply(categorize_failure)

# Function to determine failure cause for each row:
def assign_failure_cause(row):
    if row["machine_status"] == "NORMAL":
        return "no_failure"

    # Get sensors that are abnormal in this row:
    failed_sensors = [sensor for sensor in flag_columns if row[sensor] == 1]

    # Get failure categories for the failed sensors:
    failure_types = failure_categories[failed_sensors].unique()

    # Prioritize failures: Critical > Moderate:
    if "critical_failure" in failure_types:
        return "critical_failure"
    else:
        return "moderate_failure"

# Apply function to determine failure cause:
df["failure_category"] = df.apply(assign_failure_cause, axis=1)

# Display distribution of failure causes:
value_counts_with_percentage(df, 'failure_category')

"""### **Failure Cause**<a id='Failure_Cause'></a>
[Contents](#Contents)

Now that we have a high-level failure category, let's take it a step further and pinpoint where the issue is happening. Instead of just saying "critical failure", we want to know if it's a motor issue, pump issue, electrical issue, or temperature issue:

* **Motor Issue:** If motor-related sensors show abnormalities
* **Pump Issue:** If pump-related sensors are failing
* **Electrical Issue:** If voltage or current sensors are outside thresholds
* **Temperature Issue:** If any temperature sensor exceeds limits
* If **multiple issues** occur at the same time, we'll list all of them ("Motor Issue, Electrical Issue" .. etc).
"""

def classify_failure_cause(row):
    """
    Assigns failure causes based on which sensor flags are abnormal.
    If multiple failures occur, they are combined (Example: "Rotational Issue, Electrical Issue").
    """
    failure_causes = []

    # Rotational Issue (Motor & Pump Speed):
    motor_speed_flag = row["snr_03_motor_speed_flag"]
    pump_speed_flag = row["snr_04_pump_impeller_speed_flag"]

    if motor_speed_flag == 1 or pump_speed_flag == 1:
        failure_causes.append("rotational_issue")

    # Vibration Issue (Motor & Pump):
    if row["snr_08_motor_casing_vibration_flag"] == 1 or row["snr_05_pump_casing_vibration_flag"] == 1:
        failure_causes.append("vibration_issue")

    # Pressure Issue (Pump Discharge Pressure):
    if row["snr_06_pump_discharge_pressure_flag"] == 1:
        failure_causes.append("pressure_issue")

    # Temperature Issue (Lube Oil Supply Temperature):
    if row["snr_07_pump_lube_oil_supply_temp_flag"] == 1:
        failure_causes.append("temperature_ssue")

    # Electrical Issue (Motor Current, Shaft Power, Voltage):
    if (row["snr_01_motor_phase_current_flag"] == 1 or
        row["snr_02_motor_shaft_power_flag"] == 1 or
        row["snr_09_motor_phase_voltage_flag"] == 1):
        failure_causes.append("electrical_issue")

    # If machine is normal, mark it as no failure:
    if row["machine_status"] == "NORMAL":
        return "no_failure"

    # If no specific failure is detected, mark as unknown failure:
    return ", ".join(failure_causes) if failure_causes else "unknown_failure"

# Apply function to assign failure causes:
df["failure_cause"] = df.apply(classify_failure_cause, axis=1)

# Display distribution of failure causes:
value_counts_with_percentage(df, "failure_cause")

df.head()

"""### **Affected Sensors List**<a id='Affected_Sensors_List'></a>
[Contents](#Contents)

Now that we have broad failure categories, let's go one step further and list the exact sensors that triggered a failure:

* Instead of just saying "Rotational Issue", we'll show which specific speed sensors were impacted.
* This gives a detailed diagnostic view, making it easier to pinpoint the exact issue.
* If multiple sensors are abnormal, we'll list them all together in one column.
"""

def list_faulty_sensors(row):
    """
    Identifies all sensors that are in an abnormal state and lists them.
    If no sensors are abnormal, it returns 'No faulty Sensors'.
    """
    faulty_sensors = []

    # Check each sensor flag:
    for sensor in flag_columns:
        if row[sensor] == 1:
            faulty_sensors.append(sensor.replace("_flag", ""))
    return ", ".join(faulty_sensors) if faulty_sensors else "no_faulty_sensors"

# Apply function to assign impacted sensors:
df["faulty_sensors"] = df.apply(list_faulty_sensors, axis=1)

# Display distribution of faulty sensors:
value_counts_with_percentage(df, "faulty_sensors")

df[df['machine_status'] == 'ABNORMAL'].head()

"""## **Saving the clean dataset**<a id='Saving_dataset'></a>
[Contents](#Contents)
"""

# Alright, let's save the DataFrame as a CSV file:
df.to_csv('/content/drive/My Drive/Colab Notebooks/sensor_clean_df.csv', index=False)

# Here's, let's call our helper function "summarize_df" to get more overall look about the columns in the dataset:
summarize_df(df)

